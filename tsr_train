import os
import math
import logging
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import torch
from torch.utils.data import Dataset, DataLoader

from PIL import Image
from transformers import (
    VisionEncoderDecoderModel,
    ViTImageProcessor,
    GPT2TokenizerFast,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)
import sacrebleu

@dataclass
class Config:
    ROOT_DIR: str = "./TableBank/Recognition"  
    IMAGES_DIR_NAME: str = "Images"
    ANNOTATIONS_DIR_NAME: str = "Annotations"
    MODEL_OUTPUT_DIR: str = "./tsr_vit_gpt2_out"
    CHECKPOINT_DIR: str = "./checkpoints"

    ENCODER_MODEL: str = "google/vit-base-patch16-224-in21k"
    DECODER_MODEL: str = "gpt2"

    NUM_EPOCHS: int = 5
    BATCH_SIZE: int = 8
    LEARNING_RATE: float = 5e-5
    GRAD_ACCUMULATION_STEPS: int = 1
    EVAL_BATCH_SIZE: int = 8

    SEED: int = 42
    MAX_TARGET_LENGTH: int = 256
    MAX_DECODING_LENGTH: int = 256
    NUM_BEAMS: int = 4

    TRAIN_SIZE: Optional[int] = 20000  
    VAL_SIZE: Optional[int] = 5000
    TEST_SIZE: Optional[int] = 1000

    DEVICE: str = "cuda" if torch.cuda.is_available() else "cpu"

config = Config()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
)
logger = logging.getLogger(__name__)


class TableBankRecognitionDataset(Dataset):

    def __init__(
        self,
        root_dir: str,
        split: str,
        image_processor: ViTImageProcessor,
        tokenizer: GPT2TokenizerFast,
        max_target_length: int = 256,
        img_dir_name: str = "Images",
        annotations_dir_name: str = "Annotations",
        img_exts: List[str] = [".png", ".jpg", ".jpeg"],
        subset_size: Optional[int] = None,
    ):
        self.root_dir = root_dir
        self.split = split
        self.img_processor = image_processor
        self.tokenizer = tokenizer
        self.max_target_length = max_target_length
        self.img_dir = os.path.join(root_dir, img_dir_name)
        self.ann_dir = os.path.join(root_dir, annotations_dir_name)

        possible_src_files = [
            f"src-all_{split}.txt",
            f"rc-all_{split}.txt",
            f"all_{split}.txt",
            f"src_all_{split}.txt",
            f"rc_all_{split}.txt",
        ]
        possible_tgt_files = [
            f"tgt-all_{split}.txt",
            f"tgt_all_{split}.txt",
            f"tgt-all.{split}.txt",
        ]

        src_path = None
        for fn in possible_src_files:
            p = os.path.join(self.ann_dir, fn)
            if os.path.exists(p):
                src_path = p
                break
        if src_path is None:
            raise FileNotFoundError(f"No source file found for split {split} in {self.ann_dir}")

        tgt_path = None
        for fn in possible_tgt_files:
            p = os.path.join(self.ann_dir, fn)
            if os.path.exists(p):
                tgt_path = p
                break
        if tgt_path is None:
            alt = os.path.join(self.ann_dir, f"tgt_{split}.txt")
            if os.path.exists(alt):
                tgt_path = alt
            else:
                raise FileNotFoundError(f"No target file found for split {split} in {self.ann_dir}")

        logger.info(f"Using src file: {src_path}")
        logger.info(f"Using tgt file: {tgt_path}")

        with open(src_path, "r", encoding="utf-8") as f:
            src_lines = [l.strip() for l in f if l.strip()]
        with open(tgt_path, "r", encoding="utf-8") as f:
            tgt_lines = [l.rstrip("\n") for l in f]

        if len(src_lines) != len(tgt_lines):
            logger.warning(f"src ({len(src_lines)}) and tgt ({len(tgt_lines)}) line counts differ.")

        n = min(len(src_lines), len(tgt_lines))
        if subset_size is not None:
            n = min(n, subset_size)

        self.image_filenames = src_lines[:n]
        self.targets = tgt_lines[:n]
        logger.info(f"Loaded {len(self.image_filenames)} samples for split '{split}'")

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        filename = self.image_filenames[idx]
        img_path = os.path.join(self.img_dir, filename)
        if not os.path.exists(img_path):
            img_path_alt = os.path.join(self.img_dir, os.path.basename(filename))
            if os.path.exists(img_path_alt):
                img_path = img_path_alt

        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image not found: {img_path} (original: {filename})")

        image = Image.open(img_path).convert("RGB")

        px = self.img_processor(image, return_tensors="pt")
        pixel_values = px["pixel_values"].squeeze(0)

        target = self.targets[idx].strip()
        tok = self.tokenizer(
            target,
            truncation=True,
            max_length=self.max_target_length,
            padding="max_length",
            return_tensors="pt",
        )
        labels = tok["input_ids"].squeeze(0)
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {"pixel_values": pixel_values, "labels": labels, "text": target, "image_id": filename}


def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    pixel_values = torch.stack([item["pixel_values"] for item in batch], dim=0)
    labels = torch.stack([item["labels"] for item in batch], dim=0)
    return {"pixel_values": pixel_values, "labels": labels}


def compute_metrics(eval_pred):
    preds, labels = eval_pred

    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = labels.copy()
    labels[labels == -100] = tokenizer.pad_token_id
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = [p.strip() for p in decoded_preds]
    decoded_labels = [r.strip() for r in decoded_labels]

    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])
    score = bleu.score  
    return {"bleu": score}


def main():
    torch.manual_seed(config.SEED)

    global image_processor, tokenizer  
    logger.info("Loading image processor and tokenizer...")
    image_processor = ViTImageProcessor.from_pretrained(config.ENCODER_MODEL)
    tokenizer = GPT2TokenizerFast.from_pretrained(config.DECODER_MODEL, add_prefix_space=True)

    special_added = False
    add_special_tokens = {}
    if tokenizer.pad_token is None:
        add_special_tokens["pad_token"] = "<pad>"
    if tokenizer.eos_token is None:
        add_special_tokens["eos_token"] = "<|endoftext|>"
    if tokenizer.bos_token is None:
        add_special_tokens["bos_token"] = "<bos>"

    if add_special_tokens:
        tokenizer.add_special_tokens(add_special_tokens)
        special_added = True
        logger.info(f"Added special tokens to tokenizer: {add_special_tokens}")

    logger.info("Creating VisionEncoderDecoderModel from pretrained encoder+decoder...")
    model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
        config.ENCODER_MODEL, config.DECODER_MODEL
    )

    if special_added:
        model.decoder.resize_token_embeddings(len(tokenizer))

    model.config.decoder_start_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id
    model.config.eos_token_id = tokenizer.eos_token_id
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.vocab_size = model.decoder.config.vocab_size

    model.config.max_length = config.MAX_DECODING_LENGTH
    model.config.no_repeat_ngram_size = 3
    model.config.early_stopping = True
    model.config.num_beams = config.NUM_BEAMS

    device = torch.device(config.DEVICE)
    model.to(device)

    logger.info("Preparing datasets...")
    train_ds = TableBankRecognitionDataset(
        root_dir=config.ROOT_DIR,
        split="train",
        image_processor=image_processor,
        tokenizer=tokenizer,
        max_target_length=config.MAX_TARGET_LENGTH,
        subset_size=config.TRAIN_SIZE,
    )
    val_ds = TableBankRecognitionDataset(
        root_dir=config.ROOT_DIR,
        split="val",
        image_processor=image_processor,
        tokenizer=tokenizer,
        max_target_length=config.MAX_TARGET_LENGTH,
        subset_size=config.VAL_SIZE,
    )

    training_args = Seq2SeqTrainingArguments(
        output_dir=config.MODEL_OUTPUT_DIR,
        num_train_epochs=config.NUM_EPOCHS,
        per_device_train_batch_size=config.BATCH_SIZE,
        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,
        predict_with_generate=True,
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="steps",
        logging_steps=100,
        save_total_limit=3,
        learning_rate=config.LEARNING_RATE,
        gradient_accumulation_steps=config.GRAD_ACCUMULATION_STEPS,
        fp16=torch.cuda.is_available(),
        remove_unused_columns=False,  
        load_best_model_at_end=True,
        metric_for_best_model="bleu",
        greater_is_better=True,
        dataloader_pin_memory=True,
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=val_ds,
        data_collator=lambda data: collate_fn(data),
        tokenizer=tokenizer,  
        compute_metrics=compute_metrics,
    )

    logger.info("Starting training...")
    trainer.train()

    logger.info("Saving model and tokenizer...")
    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)
    trainer.save_model(config.MODEL_OUTPUT_DIR)
    tokenizer.save_pretrained(config.MODEL_OUTPUT_DIR)
    image_processor.save_pretrained(config.MODEL_OUTPUT_DIR)

    logger.info("Training complete. Model saved to %s", config.MODEL_OUTPUT_DIR)


if __name__ == "__main__":
    main()
