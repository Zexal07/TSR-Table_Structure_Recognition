import os
from pathlib import Path
from typing import Any, Dict, List

import torch
from torch.utils.data import Dataset
from PIL import Image, ImageFile
from transformers import (
    VisionEncoderDecoderModel,
    ViTImageProcessor,
    GPT2TokenizerFast,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    EvalPrediction,
)
import sacrebleu
import logging
import warnings

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.WARNING)
ImageFile.LOAD_TRUNCATED_IMAGES = True


class Config:
    ROOT_DIR = Path("./TableBank/Recognition")
    IMAGE_DIR = ROOT_DIR / "Images"
    ANNOTATION_DIR = ROOT_DIR / "Annotations"

    ENCODER_MODEL = "google/vit-base-patch16-224-in21k"
    DECODER_MODEL = "gpt2"

    NUM_EPOCHS = 3
    BATCH_SIZE = 8
    LEARNING_RATE = 5e-5
    GRAD_ACCUMULATION_STEPS = 2

    TRAIN_SIZE = 20000
    VAL_SIZE = 1000
    TEST_SIZE = 5000

    MODEL_OUTPUT_DIR = "tsr_vit_tablebank_v1"
    CHECKPOINT_DIR = "tsr_vit_tablebank_checkpoints"

    MAX_TARGET_LENGTH = 128
    GENERATION_MAX_LENGTH = 128


class TableBankRD(Dataset):
    def __init__(self, config: Config, split: str = "train"):
        self.config = config
        self.split = split
        self.image_paths: List[str] = []
        self.texts: List[str] = []

        candidates = [
            (f"src_all_{split}.txt", f"tgt_all_{split}.txt"),
            (f"src-all_{split}.txt", f"tgt-all_{split}.txt"),
            (f"all_{split}.txt", f"tgt_{split}.txt"),
            (f"src_{split}.txt", f"tgt_{split}.txt"),
            (f"src_all.txt", f"tgt_all.txt"),
        ]

        found = False
        for src_name, tgt_name in candidates:
            src_file = self.config.ANNOTATION_DIR / src_name
            tgt_file = self.config.ANNOTATION_DIR / tgt_name
            if src_file.exists() and tgt_file.exists():
                found = True
                break

        if not found:
            raise FileNotFoundError(
                f"Could not find annotation files in {self.config.ANNOTATION_DIR}. Tried: {candidates}"
            )

        with open(src_file, 'r', encoding='utf-8') as f:
            all_image_paths = [line.strip() for line in f.readlines()]

        with open(tgt_file, 'r', encoding='utf-8') as f:
            all_texts = [line.rstrip('') for line in f.readlines()]

        assert len(all_image_paths) == len(all_texts), "Annotation mismatch"

        if split == "train":
            size_limit = min(config.TRAIN_SIZE, len(all_image_paths))
        elif split == "val":
            size_limit = min(config.VAL_SIZE, len(all_image_paths))
        elif split == "test":
            size_limit = min(config.TEST_SIZE, len(all_image_paths))
        else:
            size_limit = len(all_image_paths)

        self.image_paths = all_image_paths[:size_limit]
        self.texts = all_texts[:size_limit]

        logging.info(f"{split.upper()} set: {len(self.image_paths)} samples")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        image_name = self.image_paths[idx]
        image_path = self.config.IMAGE_DIR / image_name
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            logging.warning(f"Could not load image {image_path}: {e}")
            raise
        text = self.texts[idx]
        return {"image": image, "text": text}


class DataCollator:
    def __init__(self, image_processor: ViTImageProcessor, tokenizer: GPT2TokenizerFast, cfg: Config):
        self.image_processor = image_processor
        self.tokenizer = tokenizer
        self.cfg = cfg

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        images = [item["image"] for item in batch]
        texts = [item["text"] for item in batch]

        pixel_values = self.image_processor(images=images, return_tensors="pt").pixel_values

        tokenized = self.tokenizer(
            texts,
            padding='longest',
            truncation=True,
            max_length=self.cfg.MAX_TARGET_LENGTH,
            return_tensors='pt'
        )
        labels = tokenized.input_ids
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            "pixel_values": pixel_values,
            "labels": labels,
        }


def compute_metrics(pred: EvalPrediction, tokenizer: GPT2TokenizerFast) -> Dict[str, float]:
    preds = pred.predictions
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    label_ids = pred.label_ids
    labels = label_ids.copy()
    labels[labels == -100] = tokenizer.pad_token_id
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])
    return {"bleu": float(bleu.score)}


def train_model():
    cfg = Config()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Using device: {device}")

    image_processor = ViTImageProcessor.from_pretrained(cfg.ENCODER_MODEL)
    tokenizer = GPT2TokenizerFast.from_pretrained(cfg.DECODER_MODEL)

    special_tokens = {}
    if tokenizer.pad_token is None:
        special_tokens['pad_token'] = '<pad>'
    if tokenizer.eos_token is None:
        special_tokens['eos_token'] = ''
    if tokenizer.bos_token is None:
        special_tokens['bos_token'] = '<bos>'

    if special_tokens:
        tokenizer.add_special_tokens(special_tokens)

    model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
        cfg.ENCODER_MODEL, cfg.DECODER_MODEL
    )

    model.decoder.resize_token_embeddings(len(tokenizer))

    model.config.decoder_start_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id
    model.config.eos_token_id = tokenizer.eos_token_id
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.vocab_size = model.config.decoder.vocab_size = len(tokenizer)

    model.config.is_encoder_decoder = True
    try:
        model.decoder.config.is_decoder = True
        model.decoder.config.add_cross_attention = True
    except Exception:
        pass

    gen_kwargs = {
        "max_length": cfg.GENERATION_MAX_LENGTH,
        "num_beams": 4,
        "early_stopping": True,
    }

    train_dataset = TableBankRD(cfg, split="train")
    val_dataset = TableBankRD(cfg, split="val")

    data_collator = DataCollator(image_processor, tokenizer, cfg)

    training_args = Seq2SeqTrainingArguments(
        output_dir=cfg.CHECKPOINT_DIR,
        num_train_epochs=cfg.NUM_EPOCHS,
        per_device_train_batch_size=cfg.BATCH_SIZE,
        per_device_eval_batch_size=cfg.BATCH_SIZE,
        learning_rate=cfg.LEARNING_RATE,
        gradient_accumulation_steps=cfg.GRAD_ACCUMULATION_STEPS,
        fp16=True if device.type == 'cuda' else False,
        dataloader_num_workers=0,  
        dataloader_pin_memory=True if device.type == 'cuda' else False,
        eval_strategy="epoch",
        save_strategy="epoch",
        predict_with_generate=True,
        generation_max_length=cfg.GENERATION_MAX_LENGTH,
        logging_strategy="steps",
        logging_steps=100,
        load_best_model_at_end=True,
        metric_for_best_model="bleu",
        greater_is_better=True,
        remove_unused_columns=False,
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=lambda p: compute_metrics(p, tokenizer),
    )

    trainer.model.config.update(gen_kwargs)

    try:
        trainer.train()
    except RuntimeError as e:
        logging.error(f"Training failed: {e}")
        if 'out of memory' in str(e).lower():
            logging.error("OUT OF MEMORY. Try reducing BATCH_SIZE or GENERATION_MAX_LENGTH.")
        raise

    os.makedirs(cfg.MODEL_OUTPUT_DIR, exist_ok=True)
    trainer.save_model(cfg.MODEL_OUTPUT_DIR)
    tokenizer.save_pretrained(cfg.MODEL_OUTPUT_DIR)
    image_processor.save_pretrained(cfg.MODEL_OUTPUT_DIR)
    logging.info(f"Saved to {cfg.MODEL_OUTPUT_DIR}")


if __name__ == '__main__':
    train_model()
