{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab49a4bd-78a5-4753-9c64-af1c420466f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CRNNModel:\n\tMissing key(s) in state_dict: \"cnn.3.weight\", \"cnn.3.bias\", \"cnn.6.weight\", \"cnn.6.bias\", \"cnn.7.weight\", \"cnn.7.bias\", \"cnn.7.running_mean\", \"cnn.7.running_var\", \"cnn.13.weight\", \"cnn.13.bias\", \"cnn.13.running_mean\", \"cnn.13.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"map_to_rnn.weight\", \"map_to_rnn.bias\", \"linear.weight\", \"linear.bias\", \"cnn.1.weight\", \"cnn.1.bias\", \"cnn.1.running_mean\", \"cnn.1.running_var\", \"cnn.1.num_batches_tracked\", \"cnn.4.weight\", \"cnn.4.bias\", \"cnn.5.weight\", \"cnn.5.bias\", \"cnn.5.running_mean\", \"cnn.5.running_var\", \"cnn.5.num_batches_tracked\", \"cnn.8.weight\", \"cnn.8.bias\", \"cnn.9.running_mean\", \"cnn.9.running_var\", \"cnn.9.num_batches_tracked\", \"cnn.11.weight\", \"cnn.11.bias\", \"cnn.12.running_mean\", \"cnn.12.running_var\", \"cnn.12.num_batches_tracked\", \"cnn.16.weight\", \"cnn.16.bias\", \"cnn.16.running_mean\", \"cnn.16.running_var\", \"cnn.16.num_batches_tracked\", \"cnn.19.weight\", \"cnn.19.bias\", \"cnn.19.running_mean\", \"cnn.19.running_var\", \"cnn.19.num_batches_tracked\", \"rnn.weight_ih_l2\", \"rnn.weight_hh_l2\", \"rnn.bias_ih_l2\", \"rnn.bias_hh_l2\", \"rnn.weight_ih_l2_reverse\", \"rnn.weight_hh_l2_reverse\", \"rnn.bias_ih_l2_reverse\", \"rnn.bias_hh_l2_reverse\". \n\tsize mismatch for cnn.0.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 1, 3, 3]).\n\tsize mismatch for cnn.9.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for cnn.12.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for cnn.12.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for cnn.15.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for cnn.18.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 2, 2]).\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 282\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraction failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[42], line 258\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    255\u001b[0m CRNN_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtr_crnn_mini.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m CHARSET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,-?!&\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 258\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m TableExtractionPipeline(\n\u001b[0;32m    259\u001b[0m     tsr_model_path\u001b[38;5;241m=\u001b[39mTSR_MODEL,\n\u001b[0;32m    260\u001b[0m     crnn_model_path\u001b[38;5;241m=\u001b[39mCRNN_MODEL,\n\u001b[0;32m    261\u001b[0m     charset\u001b[38;5;241m=\u001b[39mCHARSET,\n\u001b[0;32m    262\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m )\n\u001b[0;32m    265\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_image.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[42], line 202\u001b[0m, in \u001b[0;36mTableExtractionPipeline.__init__\u001b[1;34m(self, tsr_model_path, crnn_model_path, charset, device)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    196\u001b[0m     tsr_model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m ):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructure_recognizer \u001b[38;5;241m=\u001b[39m TableStructureRecognizer(tsr_model_path, device)\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_extractor \u001b[38;5;241m=\u001b[39m TextExtractor(crnn_model_path, charset, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_detector \u001b[38;5;241m=\u001b[39m CellDetector()\n",
      "Cell \u001b[1;32mIn[42], line 63\u001b[0m, in \u001b[0;36mTextExtractor.__init__\u001b[1;34m(self, model_path, charset, img_height, device)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m state_dict:\n\u001b[0;32m     62\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CRNNModel:\n\tMissing key(s) in state_dict: \"cnn.3.weight\", \"cnn.3.bias\", \"cnn.6.weight\", \"cnn.6.bias\", \"cnn.7.weight\", \"cnn.7.bias\", \"cnn.7.running_mean\", \"cnn.7.running_var\", \"cnn.13.weight\", \"cnn.13.bias\", \"cnn.13.running_mean\", \"cnn.13.running_var\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"map_to_rnn.weight\", \"map_to_rnn.bias\", \"linear.weight\", \"linear.bias\", \"cnn.1.weight\", \"cnn.1.bias\", \"cnn.1.running_mean\", \"cnn.1.running_var\", \"cnn.1.num_batches_tracked\", \"cnn.4.weight\", \"cnn.4.bias\", \"cnn.5.weight\", \"cnn.5.bias\", \"cnn.5.running_mean\", \"cnn.5.running_var\", \"cnn.5.num_batches_tracked\", \"cnn.8.weight\", \"cnn.8.bias\", \"cnn.9.running_mean\", \"cnn.9.running_var\", \"cnn.9.num_batches_tracked\", \"cnn.11.weight\", \"cnn.11.bias\", \"cnn.12.running_mean\", \"cnn.12.running_var\", \"cnn.12.num_batches_tracked\", \"cnn.16.weight\", \"cnn.16.bias\", \"cnn.16.running_mean\", \"cnn.16.running_var\", \"cnn.16.num_batches_tracked\", \"cnn.19.weight\", \"cnn.19.bias\", \"cnn.19.running_mean\", \"cnn.19.running_var\", \"cnn.19.num_batches_tracked\", \"rnn.weight_ih_l2\", \"rnn.weight_hh_l2\", \"rnn.bias_ih_l2\", \"rnn.bias_hh_l2\", \"rnn.weight_ih_l2_reverse\", \"rnn.weight_hh_l2_reverse\", \"rnn.bias_ih_l2_reverse\", \"rnn.bias_hh_l2_reverse\". \n\tsize mismatch for cnn.0.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 1, 3, 3]).\n\tsize mismatch for cnn.9.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for cnn.12.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for cnn.12.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for cnn.15.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for cnn.18.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 512, 2, 2]).\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([1024, 512])."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EvalPrediction,\n",
    ")\n",
    "import sacrebleu\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class Config:\n",
    "    ROOT_DIR = Path(\"./TableBank/Recognition\")\n",
    "    IMAGE_DIR = ROOT_DIR / \"Images\"\n",
    "    ANNOTATION_DIR = ROOT_DIR / \"Annotations\"\n",
    "    ENCODER_MODEL = \"google/vit-base-patch16-224-in21k\"\n",
    "    DECODER_MODEL = \"facebook/bart-base\"\n",
    "    NUM_EPOCHS = 8\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 5e-5\n",
    "    GRAD_ACCUMULATION_STEPS = 2\n",
    "    TRAIN_SIZE = 8000\n",
    "    VAL_SIZE = 1024\n",
    "    TEST_SIZE = 1024\n",
    "    MODEL_OUTPUT_DIR = \"tsr_vit_tablebank_v_hr\"\n",
    "    CHECKPOINT_DIR = \"tsr_vit_tablebank_checkpoints_hr\"\n",
    "    MAX_TARGET_LENGTH = 512\n",
    "    GENERATION_MAX_LENGTH = 512\n",
    "    DEBUG_MODE = False\n",
    "    SEED = 42\n",
    "    IMAGE_SIZE = 224\n",
    "    SOURCE_TYPE = \"all\"\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def normalize_html_label(s: str) -> str:\n",
    "    if s is None:\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"> <\", \"><\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_split_annotations(\n",
    "    annotation_dir: Path, source_type: str, split: str\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    src_pattern = f\"src-{source_type}_{split}.txt\"\n",
    "    tgt_pattern = f\"tgt-{source_type}_{split}.txt\"\n",
    "    \n",
    "    src_file = annotation_dir / src_pattern\n",
    "    tgt_file = annotation_dir / tgt_pattern\n",
    "    \n",
    "    if not src_file.exists():\n",
    "        raise FileNotFoundError(f\"Source file not found: {src_file}\")\n",
    "    if not tgt_file.exists():\n",
    "        raise FileNotFoundError(f\"Target file not found: {tgt_file}\")\n",
    "    \n",
    "    with open(src_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        image_paths = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    with open(tgt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = [normalize_html_label(line.rstrip(\"\\n\")) for line in f.readlines()]\n",
    "    \n",
    "    if len(image_paths) != len(texts):\n",
    "        raise ValueError(\n",
    "            f\"Mismatch in {src_pattern}/{tgt_pattern}: \"\n",
    "            f\"{len(image_paths)} images vs {len(texts)} labels\"\n",
    "        )\n",
    "    \n",
    "    logging.info(f\"Loaded {split} split: {len(image_paths)} samples from {src_pattern}\")\n",
    "    return image_paths, texts\n",
    "\n",
    "\n",
    "class TableBankDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        texts: List[str],\n",
    "        config: Config,\n",
    "        split: str = \"train\",\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        \n",
    "        if split == \"train\" and len(image_paths) > config.TRAIN_SIZE:\n",
    "            self.image_paths = image_paths[: config.TRAIN_SIZE]\n",
    "            self.texts = texts[: config.TRAIN_SIZE]\n",
    "        elif split == \"val\" and len(image_paths) > config.VAL_SIZE:\n",
    "            self.image_paths = image_paths[: config.VAL_SIZE]\n",
    "            self.texts = texts[: config.VAL_SIZE]\n",
    "        elif split == \"test\" and len(image_paths) > config.TEST_SIZE:\n",
    "            self.image_paths = image_paths[: config.TEST_SIZE]\n",
    "            self.texts = texts[: config.TEST_SIZE]\n",
    "        \n",
    "        logging.info(f\"{split.upper()} dataset: {len(self.image_paths)} samples\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        image_name = self.image_paths[idx]\n",
    "        image_path = self.config.IMAGE_DIR / image_name\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load image {image_path}: {e}\")\n",
    "            image = Image.new(\"RGB\", (self.config.IMAGE_SIZE, self.config.IMAGE_SIZE), color=(255, 255, 255))\n",
    "        \n",
    "        text = self.texts[idx]\n",
    "        return {\"image\": image, \"text\": text}\n",
    "\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(\n",
    "        self, image_processor: ViTImageProcessor, tokenizer: AutoTokenizer, cfg: Config\n",
    "    ):\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "        \n",
    "        processed = self.image_processor(images, return_tensors=\"pt\")\n",
    "        pixel_values = processed[\"pixel_values\"]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            text_target=texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.cfg.MAX_TARGET_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels = tokenized.input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        decoder_input_ids = tokenized.input_ids.clone()\n",
    "        decoder_input_ids = torch.roll(decoder_input_ids, shifts=1, dims=1)\n",
    "        decoder_input_ids[:, 0] = self.tokenizer.bos_token_id\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction, tokenizer: AutoTokenizer) -> Dict[str, float]:\n",
    "    preds = pred.predictions\n",
    "    if preds is None:\n",
    "        return {\"bleu\": 0.0}\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    preds = np.where(preds >= len(tokenizer), tokenizer.pad_token_id, preds)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    label_ids = np.array(pred.label_ids)\n",
    "    label_ids = np.where(label_ids == -100, tokenizer.pad_token_id, label_ids)\n",
    "    decoded_labels = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])\n",
    "        score = float(bleu.score)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"BLEU computation failed: {e}\")\n",
    "        score = 0.0\n",
    "    \n",
    "    return {\"bleu\": score}\n",
    "\n",
    "\n",
    "class PredictionPrinterCallback(TrainerCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        image_processor: ViTImageProcessor,\n",
    "        val_dataset: TableBankDataset,\n",
    "        cfg: Config,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.val_dataset = val_dataset\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None or len(self.val_dataset) == 0:\n",
    "            return\n",
    "        \n",
    "        sample = self.val_dataset[0]\n",
    "        img = sample[\"image\"]\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "        \n",
    "        processed = self.image_processor(img, return_tensors=\"pt\")\n",
    "        inputs = processed[\"pixel_values\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_ids = model.generate(\n",
    "                pixel_values=inputs,\n",
    "                max_length=self.cfg.GENERATION_MAX_LENGTH,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "                eos_token_id=model.config.eos_token_id,\n",
    "                pad_token_id=model.config.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        if pred_ids is None:\n",
    "            return\n",
    "        \n",
    "        pred_text = self.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "        gt_text = sample[\"text\"]\n",
    "        \n",
    "        logging.info(\"\\n\" + \"=\" * 60)\n",
    "        logging.info(\n",
    "            f\"SAMPLE PREDICTION AFTER EPOCH {state.epoch if state.epoch is not None else 0.0:.2f}\"\n",
    "        )\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(f\"MODEL OUTPUT:\\n{pred_text}\")\n",
    "        logging.info(f\"\\nGROUND TRUTH:\\n{gt_text}\")\n",
    "        logging.info(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "\n",
    "def prepare_model_and_tokenizer(cfg: Config):\n",
    "    image_processor = ViTImageProcessor.from_pretrained(cfg.ENCODER_MODEL)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.DECODER_MODEL)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "    if tokenizer.bos_token is None:\n",
    "        tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "    if tokenizer.eos_token is None:\n",
    "        tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})\n",
    "    \n",
    "    html_tokens = [\n",
    "        \"<table>\",\n",
    "        \"</table>\",\n",
    "        \"<thead>\",\n",
    "        \"</thead>\",\n",
    "        \"<tbody>\",\n",
    "        \"</tbody>\",\n",
    "        \"<tr>\",\n",
    "        \"</tr>\",\n",
    "        \"<td>\",\n",
    "        \"</td>\",\n",
    "        \"<th>\",\n",
    "        \"</th>\",\n",
    "    ]\n",
    "    tokenizer.add_tokens(html_tokens)\n",
    "    \n",
    "    model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "        cfg.ENCODER_MODEL, cfg.DECODER_MODEL\n",
    "    )\n",
    "    model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.vocab_size = len(tokenizer)\n",
    "    \n",
    "    if hasattr(model.decoder, \"config\"):\n",
    "        model.decoder.config.is_decoder = True\n",
    "        model.decoder.config.add_cross_attention = True\n",
    "    \n",
    "    return image_processor, tokenizer, model\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    cfg = Config()\n",
    "    set_seed(cfg.SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "    \n",
    "    image_processor, tokenizer, model = prepare_model_and_tokenizer(cfg)\n",
    "    \n",
    "    try:\n",
    "        train_image_paths, train_texts = load_split_annotations(\n",
    "            cfg.ANNOTATION_DIR, cfg.SOURCE_TYPE, \"train\"\n",
    "        )\n",
    "        val_image_paths, val_texts = load_split_annotations(\n",
    "            cfg.ANNOTATION_DIR, cfg.SOURCE_TYPE, \"val\"\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Annotation file error: {e}\")\n",
    "        logging.error(\n",
    "            f\"Please verify that files like 'src_{cfg.SOURCE_TYPE}_train.txt' and \"\n",
    "            f\"'tgt_{cfg.SOURCE_TYPE}_train.txt' exist in {cfg.ANNOTATION_DIR}\"\n",
    "        )\n",
    "        raise\n",
    "    \n",
    "    train_dataset = TableBankDataset(train_image_paths, train_texts, cfg, split=\"train\")\n",
    "    val_dataset = TableBankDataset(val_image_paths, val_texts, cfg, split=\"val\")\n",
    "    \n",
    "    data_collator = DataCollator(image_processor, tokenizer, cfg)\n",
    "    \n",
    "    if cfg.DEBUG_MODE:\n",
    "        debug_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=min(2, len(train_dataset)),\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "        try:\n",
    "            batch = next(iter(debug_loader))\n",
    "            logging.info(f\"pixel_values shape: {batch['pixel_values'].shape}\")\n",
    "            logging.info(f\"labels shape: {batch['labels'].shape}\")\n",
    "            logging.info(f\"decoder_input_ids shape: {batch['decoder_input_ids'].shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Debug batch creation failed: {e}\")\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=cfg.CHECKPOINT_DIR,\n",
    "        num_train_epochs=cfg.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=cfg.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=cfg.BATCH_SIZE,\n",
    "        learning_rate=cfg.LEARNING_RATE,\n",
    "        gradient_accumulation_steps=cfg.GRAD_ACCUMULATION_STEPS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=cfg.GENERATION_MAX_LENGTH,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"bleu\",\n",
    "        greater_is_better=True,\n",
    "        remove_unused_columns=False,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    \n",
    "    prediction_callback = PredictionPrinterCallback(\n",
    "        tokenizer, image_processor, val_dataset, cfg\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[prediction_callback],\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Commencing training\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except RuntimeError as e:\n",
    "        logging.error(f\"Training failed: {e}\")\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            logging.error(\n",
    "                \"OUT OF MEMORY. Reduce BATCH_SIZE or IMAGE_SIZE or \"\n",
    "                \"increase GRAD_ACCUMULATION_STEPS.\"\n",
    "            )\n",
    "        raise\n",
    "    \n",
    "    os.makedirs(cfg.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    trainer.save_model(cfg.MODEL_OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(cfg.MODEL_OUTPUT_DIR)\n",
    "    image_processor.save_pretrained(cfg.MODEL_OUTPUT_DIR)\n",
    "    logging.info(f\"Model saved to {cfg.MODEL_OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cddf5ea-121d-485d-921e-11bdcbb469db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 2:29:54, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.116102</td>\n",
       "      <td>62.036618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.102269</td>\n",
       "      <td>57.477146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.105682</td>\n",
       "      <td>52.188096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.105377</td>\n",
       "      <td>59.171404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.108770</td>\n",
       "      <td>59.041188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.108921</td>\n",
       "      <td>60.120745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:============================================================\n",
      "INFO:root:Epoch 1.00 Prediction\n",
      "INFO:root:Model Output:\n",
      "INFO:root:<thead><tr><td><td><td></tr><td><tr><td><table><td></tr></td><table><td><td></table><td><tr></tr><td><td><tr></td><td></tr><tbody></td><td><table></tr><td><table>\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:============================================================\n",
      "INFO:root:============================================================\n",
      "INFO:root:Epoch 2.00 Prediction\n",
      "INFO:root:Model Output:\n",
      "INFO:root:<thead><tr><td><td><td><table><td><tr></tr><td></table><td></table><table><td></tr></td><table><td><td><tr><td><tr><tr></tr></td><td><tr></td><td><td></tr><tbody><table><td><table></tr><td><tr><table></tr><table><td></thead><tr><td></tr><td><table><table><td></tbody>\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:============================================================\n",
      "INFO:root:============================================================\n",
      "INFO:root:Epoch 3.00 Prediction\n",
      "INFO:root:Model Output:\n",
      "INFO:root:<tbody><tr><td><td><td><table><td><tr></tr><td></table><td></table><table><td></tr><td></tr></td><table>\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:============================================================\n",
      "INFO:root:============================================================\n",
      "INFO:root:Epoch 4.00 Prediction\n",
      "INFO:root:Model Output:\n",
      "INFO:root:<tbody><tr><td><td><td><table><td></tr><td><tr><td></table><td></table><table><td><td><tr></tr><td></tr></td><td><tr></td><td><td></table></td></table><td><td></tr></table>\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:============================================================\n",
      "INFO:root:============================================================\n",
      "INFO:root:Epoch 5.00 Prediction\n",
      "INFO:root:Model Output:\n",
      "INFO:root:<thead><tr><td><td><td></tr><tbody><td><td><table><td></tr><td></tr></td><table><td><td><tr><td><tr></tr><td><td></table><td></table><table><td><table></tr><td><table>\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:============================================================\n",
      "INFO:root:============================================================\n",
      "INFO:root:Epoch 6.00 Prediction\n",
      "INFO:root:Model Output:\n",
      "INFO:root:<tbody><tr><td><td><td><table><td></tr><td><td></table></td><table><td><td><tr></tr><td></tr></td><table>\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:============================================================\n",
      "There were missing keys in the checkpoint model loaded: ['decoder.lm_head.weight'].\n",
      "INFO:root:Model saved to tsr_vit_tablebank_dos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EvalPrediction,\n",
    ")\n",
    "import sacrebleu\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class Config:\n",
    "    ROOT_DIR = Path(\"./TableBank/Recognition\")\n",
    "    IMAGE_DIR = ROOT_DIR / \"Images\"\n",
    "    ANNOTATION_DIR = ROOT_DIR / \"Annotations\"\n",
    "    MODEL_LOAD_DIR = \"tsr_vit_tablebank_v_hr\"\n",
    "    CONTINUE_OUTPUT_DIR = \"tsr_vit_tablebank_dos\"\n",
    "    CHECKPOINT_DIR = \"tsr_vit_tablebank_dos_ckpt\"\n",
    "    NUM_EPOCHS = 6\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 2e-5\n",
    "    GRAD_ACCUMULATION_STEPS = 2\n",
    "    TRAIN_SIZE = 8000\n",
    "    VAL_SIZE = 1024\n",
    "    TEST_SIZE = 1024\n",
    "    MAX_TARGET_LENGTH = 512\n",
    "    GENERATION_MAX_LENGTH = 512\n",
    "    SEED = 42\n",
    "    IMAGE_SIZE = 224\n",
    "    SOURCE_TYPE = \"all\"\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def normalize_html_label(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.replace(\"> <\", \"><\")\n",
    "\n",
    "\n",
    "def load_split_annotations(annotation_dir: Path, source_type: str, split: str) -> Tuple[List[str], List[str]]:\n",
    "    src_file = annotation_dir / f\"src-{source_type}_{split}.txt\"\n",
    "    tgt_file = annotation_dir / f\"tgt-{source_type}_{split}.txt\"\n",
    "    with open(src_file, encoding=\"utf-8\") as f:\n",
    "        images = [x.strip() for x in f if x.strip()]\n",
    "    with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "        texts = [normalize_html_label(x.rstrip(\"\\n\")) for x in f]\n",
    "    if len(images) != len(texts):\n",
    "        raise ValueError(\"Annotation and image counts mismatch\")\n",
    "    return images, texts\n",
    "\n",
    "\n",
    "class TableBankDataset(Dataset):\n",
    "    def __init__(self, paths: List[str], texts: List[str], cfg: Config, split: str):\n",
    "        self.paths = paths\n",
    "        self.texts = texts\n",
    "        self.cfg = cfg\n",
    "        if split == \"train\":\n",
    "            self.paths = self.paths[: cfg.TRAIN_SIZE]\n",
    "            self.texts = self.texts[: cfg.TRAIN_SIZE]\n",
    "        elif split == \"val\":\n",
    "            self.paths = self.paths[: cfg.VAL_SIZE]\n",
    "            self.texts = self.texts[: cfg.VAL_SIZE]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        p = self.cfg.IMAGE_DIR / self.paths[idx]\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            img = Image.new(\"RGB\", (self.cfg.IMAGE_SIZE, self.cfg.IMAGE_SIZE), (255, 255, 255))\n",
    "        return {\"image\": img, \"text\": self.texts[idx]}\n",
    "\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, image_processor: ViTImageProcessor, tokenizer: AutoTokenizer, cfg: Config):\n",
    "        self.processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        images = [x[\"image\"] for x in batch]\n",
    "        texts = [x[\"text\"] for x in batch]\n",
    "        pixels = self.processor(images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        tok = self.tokenizer(text_target=texts, padding=\"longest\", truncation=True, max_length=self.cfg.MAX_TARGET_LENGTH, return_tensors=\"pt\")\n",
    "        labels = tok.input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        dec = tok.input_ids.clone()\n",
    "        dec = torch.roll(dec, 1, 1)\n",
    "        dec[:, 0] = self.tokenizer.bos_token_id\n",
    "        return {\"pixel_values\": pixels, \"labels\": labels, \"decoder_input_ids\": dec}\n",
    "\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction, tokenizer: AutoTokenizer) -> Dict[str, float]:\n",
    "    pr = pred.predictions\n",
    "    if isinstance(pr, tuple):\n",
    "        pr = pr[0]\n",
    "    pr = np.where(pr < 0, tokenizer.pad_token_id, pr)\n",
    "    pr = np.where(pr >= len(tokenizer), tokenizer.pad_token_id, pr)\n",
    "    dp = tokenizer.batch_decode(pr, skip_special_tokens=True)\n",
    "    gt = np.where(pred.label_ids == -100, tokenizer.pad_token_id, pred.label_ids)\n",
    "    dl = tokenizer.batch_decode(gt, skip_special_tokens=True)\n",
    "    try:\n",
    "        b = sacrebleu.corpus_bleu(dp, [dl]).score\n",
    "    except Exception:\n",
    "        b = 0.0\n",
    "    return {\"bleu\": float(b)}\n",
    "\n",
    "\n",
    "class EpochPredictionCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer: AutoTokenizer, processor: ViTImageProcessor, dataset: TableBankDataset, cfg: Config):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.dataset = dataset\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None or len(self.dataset) == 0:\n",
    "            return\n",
    "        s = self.dataset[0]\n",
    "        img = s[\"image\"]\n",
    "        device = next(model.parameters()).device\n",
    "        x = self.processor(img, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            y = model.generate(pixel_values=x, max_length=self.cfg.GENERATION_MAX_LENGTH, num_beams=4, early_stopping=True, decoder_start_token_id=model.config.decoder_start_token_id, eos_token_id=model.config.eos_token_id, pad_token_id=model.config.pad_token_id)\n",
    "        pred = self.tokenizer.batch_decode(y, skip_special_tokens=True)[0]\n",
    "        gt = s[\"text\"]\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(f\"Epoch {state.epoch:.2f} Prediction\")\n",
    "        logging.info(\"Model Output:\")\n",
    "        logging.info(pred)\n",
    "        logging.info(\"Ground Truth:\")\n",
    "        logging.info(gt)\n",
    "        logging.info(\"=\" * 60)\n",
    "\n",
    "\n",
    "def prepare(cfg: Config):\n",
    "    processor = ViTImageProcessor.from_pretrained(cfg.MODEL_LOAD_DIR)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL_LOAD_DIR)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(cfg.MODEL_LOAD_DIR)\n",
    "    return processor, tokenizer, model\n",
    "\n",
    "\n",
    "def train():\n",
    "    cfg = Config()\n",
    "    set_seed(cfg.SEED)\n",
    "    processor, tokenizer, model = prepare(cfg)\n",
    "\n",
    "    tr_img, tr_txt = load_split_annotations(cfg.ANNOTATION_DIR, cfg.SOURCE_TYPE, \"train\")\n",
    "    va_img, va_txt = load_split_annotations(cfg.ANNOTATION_DIR, cfg.SOURCE_TYPE, \"val\")\n",
    "\n",
    "    train_ds = TableBankDataset(tr_img, tr_txt, cfg, \"train\")\n",
    "    val_ds = TableBankDataset(va_img, va_txt, cfg, \"val\")\n",
    "\n",
    "    collator = DataCollator(processor, tokenizer, cfg)\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=cfg.CHECKPOINT_DIR,\n",
    "        num_train_epochs=cfg.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=cfg.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=cfg.BATCH_SIZE,\n",
    "        learning_rate=cfg.LEARNING_RATE,\n",
    "        gradient_accumulation_steps=cfg.GRAD_ACCUMULATION_STEPS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=cfg.GENERATION_MAX_LENGTH,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"bleu\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    cb = EpochPredictionCallback(tokenizer, processor, val_ds, cfg)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
    "        callbacks=[cb],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    os.makedirs(cfg.CONTINUE_OUTPUT_DIR, exist_ok=True)\n",
    "    trainer.save_model(cfg.CONTINUE_OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(cfg.CONTINUE_OUTPUT_DIR)\n",
    "    processor.save_pretrained(cfg.CONTINUE_OUTPUT_DIR)\n",
    "    logging.info(f\"Model saved to {cfg.CONTINUE_OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5346f403-f52a-4c6f-a3d3-664f63d68692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using device: cuda\n",
      "INFO:root:Added 6 grammar tokens to tokenizer\n",
      "INFO:root:Train samples: 8000\n",
      "INFO:root:Validation samples: 1024\n",
      "INFO:root:Encoder frozen for initial training\n",
      "INFO:root:Epoch 0: Curriculum stage 0, max_cells=2, samples=15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 2:31:50, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.277500</td>\n",
       "      <td>0.137434</td>\n",
       "      <td>35.702048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.208900</td>\n",
       "      <td>0.114808</td>\n",
       "      <td>53.793561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.215200</td>\n",
       "      <td>0.118351</td>\n",
       "      <td>52.746965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.200700</td>\n",
       "      <td>0.115533</td>\n",
       "      <td>53.597275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 1.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:<tr><td><td><td></tbody></table><td><td></tr><td><td><table><tbody><td><td></td><td><td><td><td><td><tr><td></tr>\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 1: Curriculum stage 1, max_cells=4, samples=37\n",
      "INFO:root:Encoder unfrozen at epoch 2\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 2.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:<tr><td><td><td><table><tbody><td><td></tr><td><td></tbody></table><td><tr><td><tr></tr><td></tr></td><td><tr><table><tbody><td><tr><tr></tr></tr></tr><td><table><tbody></tr><td></tbody></table><table><tbody><td></tbody></table></tbody></table><td><td><tr></tbody></table><td></tbody></table></tr><td><tr></td><tr></tr><table><tbody><td></tr></tbody></table><td></tr> <td></tr>\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 2: Curriculum stage 2, max_cells=9999, samples=8000\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 3.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:<tr><td><td><td><table><tbody><td><td></tr><td><td></tbody></table><td><tr><td><tr></tr><td></tr></td><td><tr><table><tbody><td><tr><tr></tr></tr><td><table><tbody></tr><td></tbody></table><table><tbody><td></tbody></table></tbody></table><td><td><tr></tbody></table><td></tbody></table><tr><td></tr></tbody></table><td></tr> <td></tr>\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 3: Curriculum stage 2, max_cells=9999, samples=8000\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 4.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:<tr><td><td><td><table><tbody><td><td></tr><td><td><tr><td></tbody></table><td><tr></tr><td></tr></td><td><tr><table><tbody><td><tr><tr><td><tr></tbody></table><td></tr></tbody></table><td></tbody></table><table><tbody><td></tbody></table></tbody></table></tr><td><table><tbody><table><tbody></tr><td></tbody></table><tr></tr></tr><td><tr></td><tr></tr><table><tbody><td></tr> <td></tr>\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 4: Curriculum stage 2, max_cells=9999, samples=8000\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 5.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:,\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 5: Curriculum stage 2, max_cells=9999, samples=8000\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 6.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:,\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 6: Curriculum stage 2, max_cells=9999, samples=8000\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 7.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:,\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 7: Curriculum stage 2, max_cells=9999, samples=8000\n",
      "INFO:root:================================================================================\n",
      "INFO:root:Epoch 8.00 Sample Prediction\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Prediction:\n",
      "INFO:root:,\n",
      "INFO:root:--------------------------------------------------------------------------------\n",
      "INFO:root:Ground Truth:\n",
      "INFO:root:<table><tbody><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr><tr><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td></tr></tbody></table>\n",
      "INFO:root:================================================================================\n",
      "There were missing keys in the checkpoint model loaded: ['decoder.lm_head.weight'].\n",
      "INFO:root:Training complete. Model saved to tsr_vit_tablebank_structural\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    EvalPrediction,\n",
    ")\n",
    "import sacrebleu\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    root_dir: Path = Path(\"./TableBank/Recognition\")\n",
    "    model_load_dir: str = \"tsr_vit_tablebank_v_hr\"\n",
    "    output_dir: str = \"tsr_vit_tablebank_structural\"\n",
    "    checkpoint_dir: str = \"tsr_vit_tablebank_structural_ckpt\"\n",
    "    \n",
    "    num_epochs: int = 8\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 2e-5\n",
    "    grad_accumulation_steps: int = 2\n",
    "    train_size: int = 8000\n",
    "    val_size: int = 1024\n",
    "    max_target_length: int = 512\n",
    "    generation_max_length: int = 512\n",
    "    seed: int = 42\n",
    "    image_size: int = 224\n",
    "    source_type: str = \"all\"\n",
    "    \n",
    "    struct_penalty_weight: float = 2.0\n",
    "    start_penalty: float = 2.0\n",
    "    end_penalty: float = 2.0\n",
    "    freeze_encoder_epochs: int = 2\n",
    "    \n",
    "    curriculum_stages: List[Tuple[int, int]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.curriculum_stages is None:\n",
    "            self.curriculum_stages = [(0, 2), (1, 4), (2, 9999)]\n",
    "        self.image_dir = self.root_dir / \"Images\"\n",
    "        self.annotation_dir = self.root_dir / \"Annotations\"\n",
    "    \n",
    "    @property\n",
    "    def grammar_tokens(self) -> List[str]:\n",
    "        return [\n",
    "            \"<TABLE_START>\",\n",
    "            \"<TABLE_END>\",\n",
    "            \"<ROW_START>\",\n",
    "            \"<ROW_END>\",\n",
    "            \"<CELL_START>\",\n",
    "            \"<CELL_END>\",\n",
    "        ]\n",
    "\n",
    "\n",
    "class GrammarConverter:\n",
    "    \n",
    "    HTML_TO_GRAMMAR_MAP = {\n",
    "        r\"<table>\": \"<TABLE_START>\",\n",
    "        r\"</table>\": \"<TABLE_END>\",\n",
    "        r\"<tbody>\": \"\",\n",
    "        r\"</tbody>\": \"\",\n",
    "        r\"<thead>\": \"\",\n",
    "        r\"</thead>\": \"\",\n",
    "        r\"<tr>\": \"<ROW_START>\",\n",
    "        r\"</tr>\": \"<ROW_END>\",\n",
    "        r\"<td>\": \"<CELL_START>\",\n",
    "        r\"</td>\": \"<CELL_END>\",\n",
    "        r\"<th>\": \"<CELL_START>\",\n",
    "        r\"</th>\": \"<CELL_END>\",\n",
    "    }\n",
    "    \n",
    "    GRAMMAR_TO_HTML_MAP = {\n",
    "        \"<TABLE_START>\": \"<table><tbody>\",\n",
    "        \"<TABLE_END>\": \"</tbody></table>\",\n",
    "        \"<ROW_START>\": \"<tr>\",\n",
    "        \"<ROW_END>\": \"</tr>\",\n",
    "        \"<CELL_START>\": \"<td>\",\n",
    "        \"<CELL_END>\": \"</td>\",\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def html_to_grammar(cls, html_string: str) -> str:\n",
    "        if html_string is None:\n",
    "            return \"\"\n",
    "        \n",
    "        result = html_string.strip()\n",
    "        result = re.sub(r\"\\s+\", \" \", result)\n",
    "        result = result.replace(\"> <\", \"><\")\n",
    "        \n",
    "        for html_tag, grammar_tag in cls.HTML_TO_GRAMMAR_MAP.items():\n",
    "            result = result.replace(html_tag, grammar_tag)\n",
    "        \n",
    "        if not result.startswith(\"<TABLE_START>\"):\n",
    "            result = \"<TABLE_START>\" + result\n",
    "        if not result.endswith(\"<TABLE_END>\"):\n",
    "            result = result + \"<TABLE_END>\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @classmethod\n",
    "    def grammar_to_html(cls, grammar_string: str) -> str:\n",
    "        if grammar_string is None:\n",
    "            return \"\"\n",
    "        \n",
    "        result = grammar_string\n",
    "        for grammar_tag, html_tag in cls.GRAMMAR_TO_HTML_MAP.items():\n",
    "            result = result.replace(grammar_tag, html_tag)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class AnnotationLoader:\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_split(annotation_dir: Path, source_type: str, split: str) -> Tuple[List[str], List[str]]:\n",
    "        src_file = annotation_dir / f\"src-{source_type}_{split}.txt\"\n",
    "        tgt_file = annotation_dir / f\"tgt-{source_type}_{split}.txt\"\n",
    "        \n",
    "        if not src_file.exists() or not tgt_file.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Annotation files not found: {src_file} or {tgt_file}\"\n",
    "            )\n",
    "        \n",
    "        with open(src_file, encoding=\"utf-8\") as f:\n",
    "            images = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        with open(tgt_file, encoding=\"utf-8\") as f:\n",
    "            texts = [\n",
    "                GrammarConverter.html_to_grammar(line.rstrip(\"\\n\"))\n",
    "                for line in f\n",
    "            ]\n",
    "        \n",
    "        if len(images) != len(texts):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch: {len(images)} images vs {len(texts)} annotations\"\n",
    "            )\n",
    "        \n",
    "        return images, texts\n",
    "\n",
    "\n",
    "class TableStructureDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        grammar_texts: List[str],\n",
    "        config: TrainingConfig,\n",
    "        split: str\n",
    "    ):\n",
    "        self.image_paths = image_paths\n",
    "        self.grammar_texts = grammar_texts\n",
    "        self.config = config\n",
    "        self.split = split\n",
    "        \n",
    "        self._apply_size_limit()\n",
    "        self._precompute_complexities()\n",
    "    \n",
    "    def _apply_size_limit(self):\n",
    "        if self.split == \"train\" and len(self.image_paths) > self.config.train_size:\n",
    "            self.image_paths = self.image_paths[:self.config.train_size]\n",
    "            self.grammar_texts = self.grammar_texts[:self.config.train_size]\n",
    "        elif self.split == \"val\" and len(self.image_paths) > self.config.val_size:\n",
    "            self.image_paths = self.image_paths[:self.config.val_size]\n",
    "            self.grammar_texts = self.grammar_texts[:self.config.val_size]\n",
    "    \n",
    "    def _precompute_complexities(self):\n",
    "        self.complexities = [\n",
    "            text.count(\"<CELL_START>\")\n",
    "            for text in self.grammar_texts\n",
    "        ]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        image_path = self.config.image_dir / self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except (IOError, OSError) as e:\n",
    "            logging.warning(f\"Failed to load {image_path}: {e}\")\n",
    "            image = Image.new(\n",
    "                \"RGB\",\n",
    "                (self.config.image_size, self.config.image_size),\n",
    "                (255, 255, 255)\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text\": self.grammar_texts[idx],\n",
    "            \"complexity\": self.complexities[idx]\n",
    "        }\n",
    "    \n",
    "    def get_indices_by_complexity(self, max_cells: int) -> List[int]:\n",
    "        return [\n",
    "            idx for idx, complexity in enumerate(self.complexities)\n",
    "            if complexity <= max_cells\n",
    "        ]\n",
    "\n",
    "\n",
    "class TableDataCollator:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_processor: ViTImageProcessor,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        config: TrainingConfig\n",
    "    ):\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "    \n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "        \n",
    "        pixel_values = self.image_processor(\n",
    "            images,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"]\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            text_target=texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.config.max_target_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = tokenized.input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        decoder_input_ids = self._create_decoder_inputs(tokenized.input_ids)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "            \"decoder_input_ids\": decoder_input_ids\n",
    "        }\n",
    "    \n",
    "    def _create_decoder_inputs(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        decoder_input_ids = torch.full_like(input_ids, self.tokenizer.pad_token_id)\n",
    "        decoder_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "        decoder_input_ids[:, 0] = self.tokenizer.convert_tokens_to_ids(\"<TABLE_START>\")\n",
    "        return decoder_input_ids\n",
    "\n",
    "\n",
    "class StructuralLossTrainer(Seq2SeqTrainer):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        start_token_id: int,\n",
    "        end_token_id: int,\n",
    "        start_penalty: float,\n",
    "        end_penalty: float,\n",
    "        struct_weight: float,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.start_token_id = start_token_id\n",
    "        self.end_token_id = end_token_id\n",
    "        self.start_penalty = start_penalty\n",
    "        self.end_penalty = end_penalty\n",
    "        self.struct_weight = struct_weight\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(\n",
    "            pixel_values=inputs.get(\"pixel_values\"),\n",
    "            decoder_input_ids=inputs.get(\"decoder_input_ids\"),\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        base_loss = outputs.loss\n",
    "        \n",
    "        if not model.training or self.struct_weight == 0:\n",
    "            return (base_loss, outputs) if return_outputs else base_loss\n",
    "        \n",
    "        structural_penalty = self._compute_structural_penalty(labels, outputs.logits)\n",
    "        total_loss = base_loss + self.struct_weight * structural_penalty\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "    \n",
    "    def _compute_structural_penalty(\n",
    "        self,\n",
    "        labels: torch.Tensor,\n",
    "        logits: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        if predictions.size(1) == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device)\n",
    "        \n",
    "        start_penalties = (\n",
    "            predictions[:, 0] != self.start_token_id\n",
    "        ).float() * self.start_penalty\n",
    "        \n",
    "        valid_lengths = (labels != -100).sum(dim=1)\n",
    "        last_indices = (valid_lengths - 1).clamp(min=0)\n",
    "        batch_indices = torch.arange(predictions.size(0), device=predictions.device)\n",
    "        end_predictions = predictions[batch_indices, last_indices]\n",
    "        end_penalties = (\n",
    "            end_predictions != self.end_token_id\n",
    "        ).float() * self.end_penalty\n",
    "        \n",
    "        return (start_penalties + end_penalties).mean()\n",
    "\n",
    "\n",
    "class CurriculumLearningCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, base_dataset: TableStructureDataset):\n",
    "        self.config = config\n",
    "        self.base_dataset = base_dataset\n",
    "        self.current_subset = None\n",
    "    \n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        current_epoch = int(state.epoch) if state.epoch is not None else 0\n",
    "        \n",
    "        stage_idx = self._determine_stage(current_epoch)\n",
    "        max_cells = self.config.curriculum_stages[stage_idx][1]\n",
    "        \n",
    "        if max_cells >= 9999:\n",
    "            indices = list(range(len(self.base_dataset)))\n",
    "        else:\n",
    "            indices = self.base_dataset.get_indices_by_complexity(max_cells)\n",
    "        \n",
    "        self.current_subset = Subset(self.base_dataset, indices)\n",
    "        \n",
    "        if hasattr(train_dataloader, 'dataset'):\n",
    "            train_dataloader.dataset = self.current_subset\n",
    "        \n",
    "        logging.info(\n",
    "            f\"Epoch {current_epoch}: Curriculum stage {stage_idx}, \"\n",
    "            f\"max_cells={max_cells}, samples={len(indices)}\"\n",
    "        )\n",
    "    \n",
    "    def _determine_stage(self, current_epoch: int) -> int:\n",
    "        stage_idx = 0\n",
    "        for idx, (start_epoch, _) in enumerate(self.config.curriculum_stages):\n",
    "            if current_epoch >= start_epoch:\n",
    "                stage_idx = idx\n",
    "        return stage_idx\n",
    "\n",
    "\n",
    "class EncoderUnfreezeCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, model: VisionEncoderDecoderModel):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.unfrozen = False\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        current_epoch = int(state.epoch) if state.epoch is not None else 0\n",
    "        \n",
    "        if not self.unfrozen and current_epoch >= self.config.freeze_encoder_epochs:\n",
    "            for param in self.model.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "            self.unfrozen = True\n",
    "            logging.info(f\"Encoder unfrozen at epoch {current_epoch}\")\n",
    "\n",
    "\n",
    "class SamplePredictionCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        processor: ViTImageProcessor,\n",
    "        val_dataset: TableStructureDataset,\n",
    "        config: TrainingConfig\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.val_dataset = val_dataset\n",
    "        self.config = config\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        if model is None or len(self.val_dataset) == 0:\n",
    "            return\n",
    "        \n",
    "        sample = self.val_dataset[0]\n",
    "        prediction = self._generate_prediction(model, sample[\"image\"])\n",
    "        \n",
    "        self._log_comparison(\n",
    "            state.epoch,\n",
    "            prediction,\n",
    "            sample[\"text\"]\n",
    "        )\n",
    "    \n",
    "    def _generate_prediction(self, model, image: Image.Image) -> str:\n",
    "        device = next(model.parameters()).device\n",
    "        pixel_values = self.processor(\n",
    "            image,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"pixel_values\"].to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                max_length=self.config.generation_max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=3,\n",
    "                repetition_penalty=2.0\n",
    "            )\n",
    "        \n",
    "        prediction = self.tokenizer.batch_decode(\n",
    "            generated_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def _log_comparison(self, epoch: float, prediction: str, ground_truth: str):\n",
    "        pred_html = GrammarConverter.grammar_to_html(prediction)\n",
    "        gt_html = GrammarConverter.grammar_to_html(ground_truth)\n",
    "        \n",
    "        logging.info(\"=\" * 80)\n",
    "        logging.info(f\"Epoch {epoch:.2f} Sample Prediction\")\n",
    "        logging.info(\"-\" * 80)\n",
    "        logging.info(\"Prediction:\")\n",
    "        logging.info(pred_html)\n",
    "        logging.info(\"-\" * 80)\n",
    "        logging.info(\"Ground Truth:\")\n",
    "        logging.info(gt_html)\n",
    "        logging.info(\"=\" * 80)\n",
    "\n",
    "\n",
    "class MetricsComputer:\n",
    "    \n",
    "    def __init__(self, tokenizer: AutoTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, eval_prediction: EvalPrediction) -> Dict[str, float]:\n",
    "        predictions = eval_prediction.predictions\n",
    "        label_ids = eval_prediction.label_ids\n",
    "        \n",
    "        if predictions is None:\n",
    "            return {\"bleu\": 0.0}\n",
    "        \n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        decoded_predictions = self._decode_predictions(predictions)\n",
    "        decoded_labels = self._decode_labels(label_ids)\n",
    "        \n",
    "        bleu_score = self._compute_bleu(decoded_predictions, decoded_labels)\n",
    "        \n",
    "        return {\"bleu\": float(bleu_score)}\n",
    "    \n",
    "    def _decode_predictions(self, predictions: np.ndarray) -> List[str]:\n",
    "        predictions = np.where(\n",
    "            predictions < 0,\n",
    "            self.tokenizer.pad_token_id,\n",
    "            predictions\n",
    "        )\n",
    "        predictions = np.where(\n",
    "            predictions >= len(self.tokenizer),\n",
    "            self.tokenizer.pad_token_id,\n",
    "            predictions\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    def _decode_labels(self, label_ids: np.ndarray) -> List[str]:\n",
    "        label_ids = np.where(\n",
    "            label_ids == -100,\n",
    "            self.tokenizer.pad_token_id,\n",
    "            label_ids\n",
    "        )\n",
    "        return self.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    def _compute_bleu(\n",
    "        self,\n",
    "        predictions: List[str],\n",
    "        references: List[str]\n",
    "    ) -> float:\n",
    "        if not predictions or not references:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            return sacrebleu.corpus_bleu(predictions, [references]).score\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"BLEU computation failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "class ModelInitializer:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def initialize(self) -> Tuple[\n",
    "        ViTImageProcessor,\n",
    "        AutoTokenizer,\n",
    "        VisionEncoderDecoderModel,\n",
    "        int,\n",
    "        int\n",
    "    ]:\n",
    "        image_processor = ViTImageProcessor.from_pretrained(\n",
    "            self.config.model_load_dir\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.model_load_dir)\n",
    "        \n",
    "        self._add_grammar_tokens(tokenizer)\n",
    "        \n",
    "        model = VisionEncoderDecoderModel.from_pretrained(\n",
    "            self.config.model_load_dir\n",
    "        )\n",
    "        model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        start_token_id = tokenizer.convert_tokens_to_ids(\"<TABLE_START>\")\n",
    "        end_token_id = tokenizer.convert_tokens_to_ids(\"<TABLE_END>\")\n",
    "        \n",
    "        self._configure_model(model, tokenizer, start_token_id, end_token_id)\n",
    "        \n",
    "        return image_processor, tokenizer, model, start_token_id, end_token_id\n",
    "    \n",
    "    def _add_grammar_tokens(self, tokenizer: AutoTokenizer):\n",
    "        existing_tokens = set(tokenizer.get_vocab().keys())\n",
    "        new_tokens = [\n",
    "            token for token in self.config.grammar_tokens\n",
    "            if token not in existing_tokens\n",
    "        ]\n",
    "        \n",
    "        if new_tokens:\n",
    "            num_added = tokenizer.add_tokens(new_tokens)\n",
    "            logging.info(f\"Added {num_added} grammar tokens to tokenizer\")\n",
    "    \n",
    "    def _configure_model(\n",
    "        self,\n",
    "        model: VisionEncoderDecoderModel,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        start_token_id: int,\n",
    "        end_token_id: int\n",
    "    ):\n",
    "        model.config.decoder_start_token_id = start_token_id\n",
    "        model.config.eos_token_id = end_token_id\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.vocab_size = len(tokenizer)\n",
    "        \n",
    "        if hasattr(model.decoder, \"config\"):\n",
    "            model.decoder.config.is_decoder = True\n",
    "            model.decoder.config.add_cross_attention = True\n",
    "\n",
    "\n",
    "class TrainingOrchestrator:\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self._set_seed()\n",
    "        self._setup_device()\n",
    "    \n",
    "    def _set_seed(self):\n",
    "        np.random.seed(self.config.seed)\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.config.seed)\n",
    "    \n",
    "    def _setup_device(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logging.info(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def execute(self):\n",
    "        initializer = ModelInitializer(self.config)\n",
    "        image_processor, tokenizer, model, start_id, end_id = initializer.initialize()\n",
    "        \n",
    "        train_dataset, val_dataset = self._prepare_datasets()\n",
    "        \n",
    "        self._freeze_encoder(model)\n",
    "        \n",
    "        data_collator = TableDataCollator(image_processor, tokenizer, self.config)\n",
    "        metrics_computer = MetricsComputer(tokenizer)\n",
    "        \n",
    "        training_args = self._create_training_arguments()\n",
    "        \n",
    "        trainer = StructuralLossTrainer(\n",
    "            start_token_id=start_id,\n",
    "            end_token_id=end_id,\n",
    "            start_penalty=self.config.start_penalty,\n",
    "            end_penalty=self.config.end_penalty,\n",
    "            struct_weight=self.config.struct_penalty_weight,\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=metrics_computer,\n",
    "        )\n",
    "        \n",
    "        self._attach_callbacks(trainer, model, train_dataset, val_dataset, tokenizer, image_processor)\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        self._save_artifacts(trainer, model, tokenizer, image_processor)\n",
    "    \n",
    "    def _prepare_datasets(self) -> Tuple[TableStructureDataset, TableStructureDataset]:\n",
    "        train_paths, train_texts = AnnotationLoader.load_split(\n",
    "            self.config.annotation_dir,\n",
    "            self.config.source_type,\n",
    "            \"train\"\n",
    "        )\n",
    "        val_paths, val_texts = AnnotationLoader.load_split(\n",
    "            self.config.annotation_dir,\n",
    "            self.config.source_type,\n",
    "            \"val\"\n",
    "        )\n",
    "        \n",
    "        train_dataset = TableStructureDataset(\n",
    "            train_paths,\n",
    "            train_texts,\n",
    "            self.config,\n",
    "            \"train\"\n",
    "        )\n",
    "        val_dataset = TableStructureDataset(\n",
    "            val_paths,\n",
    "            val_texts,\n",
    "            self.config,\n",
    "            \"val\"\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Train samples: {len(train_dataset)}\")\n",
    "        logging.info(f\"Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def _freeze_encoder(self, model: VisionEncoderDecoderModel):\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        logging.info(\"Encoder frozen for initial training\")\n",
    "    \n",
    "    def _create_training_arguments(self) -> Seq2SeqTrainingArguments:\n",
    "        return Seq2SeqTrainingArguments(\n",
    "            output_dir=self.config.checkpoint_dir,\n",
    "            num_train_epochs=self.config.num_epochs,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            gradient_accumulation_steps=self.config.grad_accumulation_steps,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            dataloader_num_workers=0,\n",
    "            dataloader_pin_memory=torch.cuda.is_available(),\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            predict_with_generate=True,\n",
    "            generation_max_length=self.config.generation_max_length,\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"bleu\",\n",
    "            greater_is_better=True,\n",
    "            remove_unused_columns=False,\n",
    "            save_total_limit=2,\n",
    "            warmup_ratio=0.05,\n",
    "            no_cuda=not torch.cuda.is_available(),\n",
    "        )\n",
    "    \n",
    "    def _attach_callbacks(\n",
    "        self,\n",
    "        trainer: StructuralLossTrainer,\n",
    "        model: VisionEncoderDecoderModel,\n",
    "        train_dataset: TableStructureDataset,\n",
    "        val_dataset: TableStructureDataset,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        image_processor: ViTImageProcessor\n",
    "    ):\n",
    "        curriculum_callback = CurriculumLearningCallback(self.config, train_dataset)\n",
    "        unfreeze_callback = EncoderUnfreezeCallback(self.config, model)\n",
    "        prediction_callback = SamplePredictionCallback(\n",
    "            tokenizer,\n",
    "            image_processor,\n",
    "            val_dataset,\n",
    "            self.config\n",
    "        )\n",
    "        \n",
    "        trainer.add_callback(curriculum_callback)\n",
    "        trainer.add_callback(unfreeze_callback)\n",
    "        trainer.add_callback(prediction_callback)\n",
    "    \n",
    "    def _save_artifacts(\n",
    "        self,\n",
    "        trainer: StructuralLossTrainer,\n",
    "        model: VisionEncoderDecoderModel,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        image_processor: ViTImageProcessor\n",
    "    ):\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        \n",
    "        trainer.save_model(self.config.output_dir)\n",
    "        tokenizer.save_pretrained(self.config.output_dir)\n",
    "        image_processor.save_pretrained(self.config.output_dir)\n",
    "        \n",
    "        logging.info(f\"Training complete. Model saved to {self.config.output_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = TrainingConfig()\n",
    "    orchestrator = TrainingOrchestrator(config)\n",
    "    orchestrator.execute()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e182ee67-2bf9-40b8-a8bd-3a272e2d4525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded.\n",
      "\n",
      "=== Selected Image ===\n",
      "C:/Users/ahmed/Dropbox/PC/Desktop/Ahmed Sajid/Office - NCV/NCV - HTR/TableBank/Recognition/images/1410.7223.table_0.png\n",
      "\n",
      "Extracted HTML Tokens\n",
      "\n",
      "                                                                                                                               \n",
      "\n",
      "=== Selected Image ===\n",
      "C:/Users/ahmed/Dropbox/PC/Desktop/Ahmed Sajid/Office - NCV/NCV - HTR/TableBank/Recognition/images/1404.2843.table_0.png\n",
      "\n",
      "Extracted HTML Tokens\n",
      "\n",
      "                                                                                                                               \n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    GPT2TokenizerFast\n",
    ")\n",
    "\n",
    "MODEL_DIR = \"tsr_vit_tablebank_v1\" \n",
    "MAX_LENGTH = 128\n",
    "NUM_BEAMS = 4\n",
    "\n",
    "print(\"Loading model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_DIR).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_DIR)\n",
    "image_processor = ViTImageProcessor.from_pretrained(MODEL_DIR)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "def run_inference(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    pixel_values = image_processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        pixel_values,\n",
    "        max_length=MAX_LENGTH,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def choose_image():\n",
    "    filepath = filedialog.askopenfilename(\n",
    "        title=\"Select Table Image\",\n",
    "        filetypes=[(\"Image Files\", \"*.png *.jpg *.jpeg *.bmp *.tiff\")]\n",
    "    )\n",
    "\n",
    "    if not filepath:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        result = run_inference(filepath)\n",
    "\n",
    "        result_window = tk.Toplevel(root)\n",
    "        result_window.title(\"Extracted Table Structure\")\n",
    "\n",
    "        print(f\"\\n=== Selected Image ===\\n{filepath}\")\n",
    "\n",
    "        file_label = tk.Label(result_window, text=f\"Selected Image:\\n{filepath}\")\n",
    "        file_label.pack(pady=5)\n",
    "\n",
    "        img = Image.open(filepath)\n",
    "        img.thumbnail((300, 300))  \n",
    "        img_tk = ImageTk.PhotoImage(img)\n",
    "\n",
    "        img_label = tk.Label(result_window, image=img_tk)\n",
    "        img_label.image = img_tk  \n",
    "        img_label.pack(pady=10)\n",
    "\n",
    "        text_widget = tk.Text(result_window, wrap=\"word\", height=20, width=80)\n",
    "        text_widget.pack(padx=10, pady=10)\n",
    "        text_widget.insert(tk.END, result)\n",
    "\n",
    "        print(\"\\nExtracted HTML Tokens\\n\")\n",
    "        print(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Table Structure Recognition - TSR Inference\")\n",
    "\n",
    "select_btn = tk.Button(root, text=\"Select Table Image\", command=choose_image, width=30)\n",
    "select_btn.pack(pady=20)\n",
    "\n",
    "info_label = tk.Label(root, text=\"Choose an image to extract its table HTML structure.\")\n",
    "info_label.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c715a81-e1f2-4889-bed4-0338f8ac2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 17:17:42,751 - INFO - __main__ - Loading image processor and tokenizer / decoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa429271c904bf793f51066ed27258d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba1d70b45a448b8847bc7929016aa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2147a0b5194dd0987a3a6205be2168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 17:17:46,187 - INFO - __main__ - Added special tokens to tokenizer: {'bos_token': '<s>'}\n",
      "2025-11-30 17:17:46,188 - INFO - __main__ - Creating VisionEncoderDecoderModel from pretrained encoder+decoder...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, GitConfig, GlmConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 439\u001b[0m\n\u001b[0;32m    435\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete. Model saved to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m.\u001b[39mMODEL_OUTPUT_DIR)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 439\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[15], line 317\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    314\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded special tokens to tokenizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madd_special_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    316\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating VisionEncoderDecoderModel from pretrained encoder+decoder...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 317\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_encoder_decoder_pretrained(\n\u001b[0;32m    318\u001b[0m     config\u001b[38;5;241m.\u001b[39mENCODER_MODEL, config\u001b[38;5;241m.\u001b[39mDECODER_MODEL\n\u001b[0;32m    319\u001b[0m )\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# If tokenizer expansion changed vocab size, resize embeddings\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m special_added:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:515\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.from_encoder_decoder_pretrained\u001b[1;34m(cls, encoder_pretrained_model_name_or_path, decoder_pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_decoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m kwargs_decoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39madd_cross_attention \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    507\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    508\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_pretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not initialized as a decoder. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to initialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_pretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as a decoder, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`decoder_config` to `.from_encoder_decoder_pretrained(...)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         )\n\u001b[1;32m--> 515\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(decoder_pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_decoder)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# instantiate config with corresponding kwargs\u001b[39;00m\n\u001b[0;32m    518\u001b[0m config \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderConfig\u001b[38;5;241m.\u001b[39mfrom_encoder_decoder_configs(encoder\u001b[38;5;241m.\u001b[39mconfig, decoder\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:574\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, GitConfig, GlmConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import sacrebleu\n",
    "\n",
    "# --------------------------\n",
    "# Configuration\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    ROOT_DIR: str = \"./TableBank/Recognition\"\n",
    "    IMAGES_DIR_NAME: str = \"Images\"\n",
    "    ANNOTATIONS_DIR_NAME: str = \"Annotations\"\n",
    "    MODEL_OUTPUT_DIR: str = \"./tsr_vit_byt5_out\"\n",
    "    CHECKPOINT_DIR: str = \"./checkpoints\"\n",
    "\n",
    "    ENCODER_MODEL: str = \"google/vit-base-patch16-224-in21k\"\n",
    "    DECODER_MODEL: str = \"google/byt5-small\"   # byt5-small used as tokenizer+decoder\n",
    "\n",
    "    NUM_EPOCHS: int = 5\n",
    "    BATCH_SIZE: int = 8\n",
    "    LEARNING_RATE: float = 5e-5\n",
    "    GRAD_ACCUMULATION_STEPS: int = 1\n",
    "    EVAL_BATCH_SIZE: int = 8\n",
    "\n",
    "    SEED: int = 42\n",
    "    MAX_TARGET_LENGTH: int = 512\n",
    "    MAX_DECODING_LENGTH: int = 512\n",
    "    NUM_BEAMS: int = 4\n",
    "\n",
    "    # Increased subset sizes to give more training signal (adjust to your hardware/time).\n",
    "    TRAIN_SIZE: Optional[int] = 30000\n",
    "    VAL_SIZE: Optional[int] = 2000\n",
    "    TEST_SIZE: Optional[int] = 1000\n",
    "\n",
    "    # image handling\n",
    "    IMG_RESIZE_SIDE: int = 224  # ViT default patch size -> 224\n",
    "    ASPECT_PRESERVE: bool = True\n",
    "\n",
    "    # training stability\n",
    "    FREEZE_ENCODER_EPOCHS: int = 1  # freeze encoder for 1 epoch (helps when dataset relatively small)\n",
    "    WARMUP_STEPS: int = 2000\n",
    "    MAX_GRAD_NORM: float = 1.0\n",
    "\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# --------------------------\n",
    "# Logging\n",
    "# --------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --------------------------\n",
    "# Utils: aspect preserving resize + pad\n",
    "# --------------------------\n",
    "def resize_and_pad_to_square(image: Image.Image, size: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Resize an image keeping aspect ratio, then pad to a square of side `size`.\n",
    "    Pads with white background.\n",
    "    \"\"\"\n",
    "    # Convert to RGB if not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Preserve aspect ratio\n",
    "    image.thumbnail((size, size), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Create white background and paste centered\n",
    "    new_im = Image.new(\"RGB\", (size, size), (255, 255, 255))\n",
    "    paste_x = (size - image.width) // 2\n",
    "    paste_y = (size - image.height) // 2\n",
    "    new_im.paste(image, (paste_x, paste_y))\n",
    "    return new_im\n",
    "\n",
    "# --------------------------\n",
    "# Dataset\n",
    "# --------------------------\n",
    "class TableBankRecognitionDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        split: str,\n",
    "        image_processor: ViTImageProcessor,\n",
    "        tokenizer: Any,\n",
    "        max_target_length: int = 256,\n",
    "        img_dir_name: str = \"Images\",\n",
    "        annotations_dir_name: str = \"Annotations\",\n",
    "        img_exts: List[str] = [\".png\", \".jpg\", \".jpeg\"],\n",
    "        subset_size: Optional[int] = None,\n",
    "    ):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.img_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_target_length = max_target_length\n",
    "        self.img_dir = os.path.join(root_dir, img_dir_name)\n",
    "        self.ann_dir = os.path.join(root_dir, annotations_dir_name)\n",
    "\n",
    "        possible_src_files = [\n",
    "            f\"src-all_{split}.txt\",\n",
    "            f\"rc-all_{split}.txt\",\n",
    "            f\"all_{split}.txt\",\n",
    "            f\"src_all_{split}.txt\",\n",
    "            f\"rc_all_{split}.txt\",\n",
    "        ]\n",
    "        possible_tgt_files = [\n",
    "            f\"tgt-all_{split}.txt\",\n",
    "            f\"tgt_all_{split}.txt\",\n",
    "            f\"tgt-all.{split}.txt\",\n",
    "        ]\n",
    "\n",
    "        src_path = None\n",
    "        for fn in possible_src_files:\n",
    "            p = os.path.join(self.ann_dir, fn)\n",
    "            if os.path.exists(p):\n",
    "                src_path = p\n",
    "                break\n",
    "        if src_path is None:\n",
    "            raise FileNotFoundError(f\"No source file found for split {split} in {self.ann_dir}\")\n",
    "\n",
    "        tgt_path = None\n",
    "        for fn in possible_tgt_files:\n",
    "            p = os.path.join(self.ann_dir, fn)\n",
    "            if os.path.exists(p):\n",
    "                tgt_path = p\n",
    "                break\n",
    "        if tgt_path is None:\n",
    "            alt = os.path.join(self.ann_dir, f\"tgt_{split}.txt\")\n",
    "            if os.path.exists(alt):\n",
    "                tgt_path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"No target file found for split {split} in {self.ann_dir}\")\n",
    "\n",
    "        logger.info(f\"Using src file: {src_path}\")\n",
    "        logger.info(f\"Using tgt file: {tgt_path}\")\n",
    "\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            src_lines = [l.strip() for l in f if l.strip()]\n",
    "        with open(tgt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            tgt_lines = [l.rstrip(\"\\n\") for l in f]\n",
    "\n",
    "        if len(src_lines) != len(tgt_lines):\n",
    "            logger.warning(f\"src ({len(src_lines)}) and tgt ({len(tgt_lines)}) line counts differ.\")\n",
    "\n",
    "        n = min(len(src_lines), len(tgt_lines))\n",
    "        if subset_size is not None:\n",
    "            n = min(n, subset_size)\n",
    "\n",
    "        self.image_filenames = src_lines[:n]\n",
    "        self.targets = tgt_lines[:n]\n",
    "        logger.info(f\"Loaded {len(self.image_filenames)} samples for split '{split}'\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        filename = self.image_filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "        if not os.path.exists(img_path):\n",
    "            img_path_alt = os.path.join(self.img_dir, os.path.basename(filename))\n",
    "            if os.path.exists(img_path_alt):\n",
    "                img_path = img_path_alt\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path} (original: {filename})\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Aspect-preserving resize + pad to square (ViT expects square)\n",
    "        if config.ASPECT_PRESERVE:\n",
    "            image = resize_and_pad_to_square(image, config.IMG_RESIZE_SIDE)\n",
    "\n",
    "        px = self.img_processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = px[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        target = self.targets[idx].strip()\n",
    "        tok = self.tokenizer(\n",
    "            target,\n",
    "            truncation=True,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = tok[\"input_ids\"].squeeze(0)\n",
    "        # set pad token ids to -100 for ignoring in loss\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels, \"text\": target, \"image_id\": filename}\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch], dim=0)\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch], dim=0)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# HTML structure metric\n",
    "# --------------------------\n",
    "TAG_RE = re.compile(r\"</?([a-zA-Z0-9_\\-]+)\")\n",
    "\n",
    "def extract_tag_sequence(html: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract ordered tag names from html, ignoring attributes and text.\n",
    "    e.g. \"<table><tr><td>1</td></tr></table>\" -> [\"table\", \"tr\", \"td\", \"/td\", \"/tr\", \"/table\"]\n",
    "    We'll capture opening and closing with slash prefix for clarity.\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    for m in TAG_RE.finditer(html):\n",
    "        full = m.group(0)\n",
    "        name = m.group(1).lower()\n",
    "        if full.startswith(\"</\"):\n",
    "            tags.append(f\"/{name}\")\n",
    "        else:\n",
    "            tags.append(name)\n",
    "    return tags\n",
    "\n",
    "def html_structure_accuracy_batch(preds: List[str], refs: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute proportion of exact-matching tag sequences in the batch.\n",
    "    This is a conservative structural accuracy: tag sequence must match exactly.\n",
    "    \"\"\"\n",
    "    assert len(preds) == len(refs)\n",
    "    matches = 0\n",
    "    for p, r in zip(preds, refs):\n",
    "        pt = extract_tag_sequence(p)\n",
    "        rt = extract_tag_sequence(r)\n",
    "        if pt == rt:\n",
    "            matches += 1\n",
    "    return matches / len(preds) if len(preds) > 0 else 0.0\n",
    "\n",
    "# --------------------------\n",
    "# Metrics used by Trainer\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    eval_pred: tuple(predictions, label_ids)\n",
    "    predictions may be generated sequences (token ids), or logits depending on call\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # If Trainer called with predict_with_generate=True, preds are decoded token ids\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # prepare labels\n",
    "    labels = labels.copy()\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # strip\n",
    "    decoded_preds = [p.strip() for p in decoded_preds]\n",
    "    decoded_labels = [r.strip() for r in decoded_labels]\n",
    "\n",
    "    # BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])\n",
    "    bleu_score = bleu.score\n",
    "\n",
    "    # HTML structure accuracy\n",
    "    struct_acc = html_structure_accuracy_batch(decoded_preds, decoded_labels)\n",
    "\n",
    "    return {\"bleu\": bleu_score, \"html_structure_acc\": struct_acc}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Main training pipeline\n",
    "# --------------------------\n",
    "def main():\n",
    "    torch.manual_seed(config.SEED)\n",
    "\n",
    "    global image_processor, tokenizer\n",
    "\n",
    "    logger.info(\"Loading image processor and tokenizer / decoder...\")\n",
    "    image_processor = ViTImageProcessor.from_pretrained(config.ENCODER_MODEL)\n",
    "    # ByT5 is an encoder-decoder model; using AutoTokenizer ensures correct tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.DECODER_MODEL, use_fast=True)\n",
    "\n",
    "    # If tokenizer has no pad or bos/eos, add them (ByT5 should have pad, eos)\n",
    "    add_special_tokens = {}\n",
    "    if tokenizer.pad_token is None:\n",
    "        add_special_tokens[\"pad_token\"] = \"<pad>\"\n",
    "    if tokenizer.eos_token is None:\n",
    "        add_special_tokens[\"eos_token\"] = \"</s>\"\n",
    "    if tokenizer.bos_token is None:\n",
    "        add_special_tokens[\"bos_token\"] = \"<s>\"\n",
    "\n",
    "    special_added = False\n",
    "    if add_special_tokens:\n",
    "        tokenizer.add_special_tokens(add_special_tokens)\n",
    "        special_added = True\n",
    "        logger.info(f\"Added special tokens to tokenizer: {add_special_tokens}\")\n",
    "\n",
    "    logger.info(\"Creating VisionEncoderDecoderModel from pretrained encoder+decoder...\")\n",
    "    model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "        config.ENCODER_MODEL, config.DECODER_MODEL\n",
    "    )\n",
    "\n",
    "    # If tokenizer expansion changed vocab size, resize embeddings\n",
    "    if special_added:\n",
    "        model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Configure generation and decoding\n",
    "    model.config.decoder_start_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # ensure vocab size is set\n",
    "    model.config.vocab_size = model.decoder.config.vocab_size\n",
    "\n",
    "    model.config.max_length = config.MAX_DECODING_LENGTH\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model.config.early_stopping = True\n",
    "    model.config.num_beams = config.NUM_BEAMS\n",
    "\n",
    "    device = torch.device(config.DEVICE)\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare datasets\n",
    "    logger.info(\"Preparing datasets...\")\n",
    "    train_ds = TableBankRecognitionDataset(\n",
    "        root_dir=config.ROOT_DIR,\n",
    "        split=\"train\",\n",
    "        image_processor=image_processor,\n",
    "        tokenizer=tokenizer,\n",
    "        max_target_length=config.MAX_TARGET_LENGTH,\n",
    "        subset_size=config.TRAIN_SIZE,\n",
    "    )\n",
    "    val_ds = TableBankRecognitionDataset(\n",
    "        root_dir=config.ROOT_DIR,\n",
    "        split=\"val\",\n",
    "        image_processor=image_processor,\n",
    "        tokenizer=tokenizer,\n",
    "        max_target_length=config.MAX_TARGET_LENGTH,\n",
    "        subset_size=config.VAL_SIZE,\n",
    "    )\n",
    "\n",
    "    # Optionally freeze encoder for initial epochs\n",
    "    if config.FREEZE_ENCODER_EPOCHS > 0:\n",
    "        logger.info(f\"Freezing encoder parameters for first {config.FREEZE_ENCODER_EPOCHS} epoch(s).\")\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Training args\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,\n",
    "        predict_with_generate=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        save_total_limit=3,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        gradient_accumulation_steps=config.GRAD_ACCUMULATION_STEPS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        remove_unused_columns=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"html_structure_acc\",\n",
    "        greater_is_better=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        warmup_steps=config.WARMUP_STEPS,\n",
    "        fp16_opt_level=\"O1\" if torch.cuda.is_available() else None,\n",
    "        max_grad_norm=config.MAX_GRAD_NORM,\n",
    "        predict_with_generate_kwargs={\"max_length\": config.MAX_DECODING_LENGTH, \"num_beams\": config.NUM_BEAMS},\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=lambda data: collate_fn(data),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Unfreeze encoder after initial epoch by a callback-style check (Trainer does not have easy epoch callbacks here).\n",
    "    # We'll implement a simple wrapper around train to unfreeze after one epoch if requested.\n",
    "    logger.info(\"Starting training...\")\n",
    "\n",
    "    if config.FREEZE_ENCODER_EPOCHS <= 0:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        # train for the freeze epochs, then unfreeze and continue\n",
    "        # NOTE: this splits the training into two calls - still uses the same Trainer object\n",
    "        initial_epochs = min(config.FREEZE_ENCODER_EPOCHS, config.NUM_EPOCHS)\n",
    "        remaining_epochs = config.NUM_EPOCHS - initial_epochs\n",
    "\n",
    "        # Train initial epochs (we accomplish this via setting num_train_epochs temporarily)\n",
    "        saved_num_epochs = trainer.args.num_train_epochs\n",
    "        trainer.args.num_train_epochs = initial_epochs\n",
    "        trainer.train()\n",
    "        # Unfreeze encoder\n",
    "        logger.info(\"Unfreezing encoder parameters for remaining epochs.\")\n",
    "        for param in model.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        # Reset the Trainer's args to the remaining epochs\n",
    "        trainer.args.num_train_epochs = remaining_epochs\n",
    "        if remaining_epochs > 0:\n",
    "            trainer.train()\n",
    "\n",
    "        # restore original\n",
    "        trainer.args.num_train_epochs = saved_num_epochs\n",
    "\n",
    "    logger.info(\"Saving model and tokenizer...\")\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    trainer.save_model(config.MODEL_OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(config.MODEL_OUTPUT_DIR)\n",
    "    image_processor.save_pretrained(config.MODEL_OUTPUT_DIR)\n",
    "\n",
    "    logger.info(\"Training complete. Model saved to %s\", config.MODEL_OUTPUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdea9214-7201-4af8-8268-2cfff9f52484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, GPT2TokenizerFast\n",
    "\n",
    "MODEL_DIR = \"./tsr_vit_gpt2_out/checkpoint-1250\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_DIR).to(device)\n",
    "image_processor = ViTImageProcessor.from_pretrained(MODEL_DIR)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_DIR)\n",
    "\n",
    "model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "def predict_structure(image_path):\n",
    "    \"\"\"Run TSR model on a single image.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except:\n",
    "        return \"Error: Could not open image.\"\n",
    "\n",
    "    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        pixel_values,\n",
    "        max_length=256,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return pred.strip()\n",
    "\n",
    "class TSRGUI:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Table Structure Recognition - ViT-GPT2\")\n",
    "\n",
    "        self.img_label = tk.Label(master, text=\"Select an image to begin.\")\n",
    "        self.img_label.pack(pady=10)\n",
    "\n",
    "        self.select_button = tk.Button(master, text=\"Choose Image\", command=self.select_image)\n",
    "        self.select_button.pack()\n",
    "\n",
    "        self.output_text = tk.Text(master, height=10, width=80)\n",
    "        self.output_text.pack(pady=10)\n",
    "\n",
    "    def select_image(self):\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select an image\",\n",
    "            filetypes=[(\"Images\", \"*.png *.jpg *.jpeg *.bmp *.tiff\")]\n",
    "        )\n",
    "\n",
    "        if not file_path:\n",
    "            return\n",
    "\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail((400, 400))\n",
    "        img_tk = ImageTk.PhotoImage(img)\n",
    "\n",
    "        self.img_label.configure(image=img_tk)\n",
    "        self.img_label.image = img_tk  \n",
    "\n",
    "        prediction = predict_structure(file_path)\n",
    "\n",
    "        self.output_text.delete(\"1.0\", tk.END)\n",
    "        self.output_text.insert(tk.END, prediction)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    gui = TSRGUI(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524c42a-b622-48cc-91a4-6cd34b2b3a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU Second)",
   "language": "python",
   "name": "gpu-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
