{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb59e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.78it/s, loss=0.54]\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.73it/s, loss=0.293]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.85it/s, loss=0.265]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.73it/s, loss=0.239]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.91it/s, loss=0.247]\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.73it/s, loss=0.222]\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.72it/s, loss=0.216]\n",
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.77it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE: 1712.00077.table_0.png\n",
      "PRED : <table> <thead> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> \n",
      "GT   : <table> <thead> <tr> <td> <td> </tr> </thead> <tbody> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n",
      "IMAGE: 1809.07129.table_1.png\n",
      "PRED : <table> <thead> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <\n",
      "GT   : <table> <thead> <tr> <td> <td> <td> <td> </tr> </thead> <tbody> <tr> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n",
      "IMAGE: 1703.04156.table_1.png\n",
      "PRED : <table> <thead> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> \n",
      "GT   : <table> <thead> <tr> <td> <td> <td> </tr> </thead> <tbody> <tr> <td> <td> <td> </tr> <tr> <td> <td> <td> </tr> <tr> <td> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n",
      "IMAGE: 1409.4936.table_1.png\n",
      "PRED : <table> <thead> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <\n",
      "GT   : <table> <tbody> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n",
      "IMAGE: 1807.08954.table_0.png\n",
      "PRED : <table> <tbody> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <\n",
      "GT   : <table> <tbody> <tr> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n",
      "IMAGE: 1606.03980.table_3.png\n",
      "PRED : <table> <thead> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <\n",
      "GT   : <table> <thead> <tr> <td> <td> <td> <td> <td> </tr> </thead> <tbody> <tr> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n",
      "IMAGE: 1505.00359.table_2.png\n",
      "PRED : <table> <thead> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <\n",
      "GT   : <table> <thead> <tr> <td> <td> </tr> </thead> <tbody> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> <tr> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n",
      "IMAGE: 1602.02241.table_6.png\n",
      "PRED : <table> <tbody> <tr> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <\n",
      "GT   : <table> <tbody> <tr> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> <tr> <td> <td> <td> <td> <td> <td> <td> <td> </tr> </tbody> </table>\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TABLEBANK_ROOT = Path(\n",
    "    r\"C:\\Users\\ahmed\\Dropbox\\PC\\Desktop\\Ahmed Sajid\\Office - NCV\\NCV - HTR\\TableBank\\Recognition\"\n",
    ")\n",
    "IMAGES_DIR = TABLEBANK_ROOT / \"Images\"\n",
    "ANNOT_DIR = TABLEBANK_ROOT / \"Annotations\"\n",
    "SRC_FILE = ANNOT_DIR / \"src-all_train.txt\"\n",
    "TGT_FILE = ANNOT_DIR / \"tgt-all_train.txt\"\n",
    "CASCADE_ROOT = Path(\n",
    "    r\"C:\\Users\\ahmed\\Dropbox\\PC\\Desktop\\Ahmed Sajid\\Office - NCV\\NCV - HTR\\CascadeTabNet\"\n",
    ")\n",
    "SUBSET_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "IMG_H = 64\n",
    "IMG_W = 512\n",
    "EMBED_DIM = 512\n",
    "NUM_EPOCHS = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_SEQ_LEN = 512\n",
    "MODEL_SAVE_PATH = Path.cwd() / \"tsr_castabnet_tablebank.pth\"\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def resolve_image_path(entry: str, images_dir: Path) -> Path:\n",
    "    p = Path(entry.strip())\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p\n",
    "    candidate = images_dir / entry.strip()\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    stripped = entry.strip().lstrip(\"./\")\n",
    "    candidate = images_dir / stripped\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    parts = entry.strip().split(\"/\")\n",
    "    candidate = images_dir / parts[-1]\n",
    "    if candidate.exists():\n",
    "        return candidate\n",
    "    raise FileNotFoundError(entry)\n",
    "\n",
    "\n",
    "class TableBankDataset(Dataset):\n",
    "    def __init__(self, src_path: Path, tgt_path: Path, images_dir: Path, max_samples: int = None):\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            srcs = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
    "        with open(tgt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            tgts = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
    "        pairs = []\n",
    "        for s, t in zip(srcs, tgts):\n",
    "            try:\n",
    "                img_path = resolve_image_path(s, images_dir)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            pairs.append((str(img_path), t))\n",
    "        if max_samples:\n",
    "            pairs = pairs[: max_samples]\n",
    "        self.pairs = pairs\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((IMG_H, IMG_W)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        chars = set()\n",
    "        for _, tgt in pairs:\n",
    "            chars.update(list(tgt))\n",
    "        self.vocab = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"] + sorted(chars)\n",
    "        self.stoi = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.itos = self.vocab\n",
    "\n",
    "    def encode_target(self, text: str) -> List[int]:\n",
    "        seq = [self.stoi[\"<bos>\"]]\n",
    "        for ch in text:\n",
    "            seq.append(self.stoi.get(ch, self.stoi[\"<unk>\"]))\n",
    "            if len(seq) >= MAX_SEQ_LEN - 1:\n",
    "                break\n",
    "        seq.append(self.stoi[\"<eos>\"])\n",
    "        return seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path, tgt = self.pairs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        tgt_ids = self.encode_target(tgt)\n",
    "        tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long)\n",
    "        return img, tgt_tensor, tgt, os.path.basename(img_path)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tgts, raw, names = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    lengths = [t.shape[0] for t in tgts]\n",
    "    max_len = max(lengths)\n",
    "    padded = torch.full((len(tgts), max_len), 0, dtype=torch.long)\n",
    "    for i, t in enumerate(tgts):\n",
    "        padded[i, : t.shape[0]] = t\n",
    "    return imgs, padded, lengths, raw, names\n",
    "\n",
    "\n",
    "def try_add_cascade_path(root: Path):\n",
    "    sys.path.insert(0, str(root))\n",
    "    sys.path.insert(0, str(root / \"Table Structure Recognition\"))\n",
    "\n",
    "\n",
    "try_add_cascade_path(CASCADE_ROOT)\n",
    "CASCADE_AVAILABLE = False\n",
    "try:\n",
    "    from CascadeTabNet.model import CascadeTabNet  # type: ignore\n",
    "    CASCADE_AVAILABLE = True\n",
    "except Exception:\n",
    "    CASCADE_AVAILABLE = False\n",
    "\n",
    "\n",
    "class EncoderWrapper(nn.Module):\n",
    "    def __init__(self, out_dim: int):\n",
    "        super().__init__()\n",
    "        r = models.resnet18(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(r.children())[:-2])\n",
    "        self.proj = nn.Linear(512, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.encoder(x)\n",
    "        b, c, h, w = f.size()\n",
    "        f = f.view(b, c, -1).permute(0, 2, 1)\n",
    "        f = self.proj(f)\n",
    "        f = f.permute(1, 0, 2)\n",
    "        return f\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(pos * div[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[: x.size(0)]\n",
    "\n",
    "\n",
    "def generate_mask(sz: int):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderWrapper(embed_dim)\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=3)\n",
    "        self.output_fc = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, imgs, tgt_seq):\n",
    "        memory = self.encoder(imgs)\n",
    "        tgt_emb = self.token_emb(tgt_seq)\n",
    "        tgt_emb = tgt_emb * math.sqrt(self.embed_dim)\n",
    "        tgt_emb = tgt_emb.permute(1, 0, 2)\n",
    "        tgt_emb = self.pos_enc(tgt_emb)\n",
    "        tgt_mask = generate_mask(tgt_emb.size(0)).to(tgt_emb.device)\n",
    "        out = self.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "        logits = self.output_fc(out)\n",
    "        return logits.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    dataset = TableBankDataset(SRC_FILE, TGT_FILE, IMAGES_DIR, max_samples=SUBSET_SIZE)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    model = Seq2SeqModel(len(dataset.vocab), EMBED_DIM).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        model.train()\n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch}\", leave=True)\n",
    "        total_loss = 0.0\n",
    "        for imgs, tgt_padded, _, _, _ in loop:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            tgt_in = tgt_padded[:, :-1].to(DEVICE)\n",
    "            tgt_out = tgt_padded[:, 1:].to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs, tgt_in)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=total_loss / (loop.n + 1))\n",
    "    torch.save({\"model_state_dict\": model.state_dict(), \"vocab\": dataset.vocab}, MODEL_SAVE_PATH)\n",
    "    return model, dataset\n",
    "\n",
    "\n",
    "def greedy_decode(model, img_tensor, stoi, itos, max_len=MAX_SEQ_LEN):\n",
    "    model.eval()\n",
    "    bos = stoi[\"<bos>\"]\n",
    "    eos = stoi[\"<eos>\"]\n",
    "    ys = torch.tensor([[bos]], device=DEVICE, dtype=torch.long)\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        memory = model.encoder(img_tensor)\n",
    "        for _ in range(max_len):\n",
    "            emb = model.token_emb(ys)\n",
    "            emb = emb * math.sqrt(model.embed_dim)\n",
    "            emb_t = emb.permute(1, 0, 2)\n",
    "            emb_t = model.pos_enc(emb_t)\n",
    "            tgt_mask = generate_mask(emb_t.size(0)).to(DEVICE)\n",
    "            out = model.decoder(emb_t, memory, tgt_mask=tgt_mask)\n",
    "            last = out[-1, 0, :]\n",
    "            logits = model.output_fc(last)\n",
    "            next_tok = logits.argmax(dim=-1)\n",
    "            next_tok_tensor = next_tok.unsqueeze(0).unsqueeze(0)\n",
    "            ys = torch.cat([ys, next_tok_tensor.to(DEVICE)], dim=1)\n",
    "            if next_tok.item() == eos:\n",
    "                break\n",
    "    tokens = ys.squeeze(0).tolist()\n",
    "    tokens = [t for t in tokens if t not in (bos, eos, 0)]\n",
    "    text = \"\".join([itos[t] if t < len(itos) else \"<unk>\" for t in tokens])\n",
    "    return text\n",
    "\n",
    "\n",
    "def quick_test(model, dataset, n=5):\n",
    "    tf = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMG_H, IMG_W)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    sample_ids = random.sample(range(len(dataset)), min(n, len(dataset)))\n",
    "    for i in sample_ids:\n",
    "        img_path, tgt = dataset.pairs[i]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_t = tf(img)\n",
    "        pred = greedy_decode(model, img_t, dataset.stoi, dataset.itos)\n",
    "        print(\"IMAGE:\", os.path.basename(img_path))\n",
    "        print(\"PRED :\", pred)\n",
    "        print(\"GT   :\", tgt)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, ds = train_model()\n",
    "    quick_test(model, ds, n=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750d30a-0c19-4eff-9a1c-b31423ddaa54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU Second)",
   "language": "python",
   "name": "gpu-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
